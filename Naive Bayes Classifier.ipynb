{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier - Explainable Machine Learning WS 23/24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 1. Einführung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes Classifier Intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "png_path = '/Users/fabian/Desktop/datasets/BayesIntro.png'\n",
    "\n",
    "Image(filename=png_path, width=400, height=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gliederung\n",
    "\n",
    "<html>\n",
    "<ol>\n",
    " <li style=\"font-style: italic;\">Einführung</li>\n",
    " <li style=\"font-style: italic;\">Preprocessing</li>\n",
    " <li style=\"font-style: italic;\">Building the Model</li>\n",
    "<li style=\"font-style: italic;\">Model Training</li>\n",
    "<li style=\"font-style: italic;\">Explainability</li>\n",
    "<li style=\"font-style: italic;\">Predictions and Evaluations</li>\n",
    "<li style=\"font-style: italic;\">Hyperparametertuning</li>\n",
    "<li style=\"font-style: italic;\">Principal Component Analysis</li>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/eva-f00/bayes_xml/blob/main/Bayes_Poster.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "png_path = '/Users/fabian/Desktop/datasets/Bayes_Poster.jpg'\n",
    "\n",
    "#Image(filename=png_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 2. Imports, Loading Dataset & Analyze Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in the following code, a Gaussian Naive Bayes Classifier will be build to predict whether a person makes over 50K a year\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv), Python data analysis library\n",
    "import matplotlib.pyplot as plt # for data visualization purposes\n",
    "import seaborn as sns # for statistical data visualization; to explore the purpose and target column\n",
    "\n",
    "# Machine Learning and data analysis\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dataset as ds\n",
    "\n",
    "#path = 'C:/Users/evafi/bayes_xml/dataset/adult_income_dataset.csv'\n",
    "path = '/Users/fabian/Desktop/datasets/adult.csv'\n",
    "#path = 'C:\\\\Users\\\\Natal\\\\Documents\\\\Wirtschaftsinformatik_Master\\\\2.Semester_WS2023-24\\\\Explainable Machine Learning\\\\Prüfungsleistung\\\\adult income dataset.csv'\n",
    "\n",
    "data = pd.read_csv(path, sep=\",\")\n",
    "\n",
    "#top 5 of each column\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Analyze the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape dataset\n",
    "#numer of rows and columns/ features\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data analysis\n",
    "n_records = data.shape[0]\n",
    "n_greater_50k = data[data['income'] == '>50K'].shape[0]\n",
    "n_at_most_50k = data[data['income'] == '<=50K'].shape[0]\n",
    "greater_percent = (n_greater_50k / n_records) * 100\n",
    "print(\"Total numbber of records: {}\".format(n_records))\n",
    "print(\"Individuals making more than $50.000: {}\".format(n_greater_50k))\n",
    "print(\"Individuals making at most $50.000: {}\".format(n_at_most_50k))\n",
    "print(\"Percentage of individuals making more than $50.000: {}%\".format(greater_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename column names\n",
    "col_names = ['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status', 'occupation', 'relationship',\n",
    "             'race', 'sex', 'capital_gain', 'capital_loss', 'hours_per_week', 'native_country', 'income']\n",
    "data.columns = col_names\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general inforamtion\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Overview of the data\n",
    "#data.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>2. Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Check values in each variable and replace them / Dealing with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check \"?\" in dataset\n",
    "\n",
    "col_n = data.columns\n",
    "num_data = data.shape[0]\n",
    "for c in col_n:\n",
    "    num_non = data[c].isin([\"?\"]).sum()\n",
    "    if num_non > 0:\n",
    "        print(c)\n",
    "        print(num_non)\n",
    "        print(\"{0:.2f}%\".format(float(num_non) / num_data * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace '?' values in workclass variable with `NaN`\n",
    "data['workclass'].replace('?', np.NaN, inplace=True)\n",
    "\n",
    "# replace '?' values in occupation variable with `NaN`\n",
    "data['occupation'].replace('?', np.NaN, inplace=True)\n",
    "\n",
    "# replace '?' values in native_country variable with `NaN`\n",
    "data['native_country'].replace('?', np.NaN, inplace=True)\n",
    "\n",
    "# impute missing categorical variables with most frequent value\n",
    "\n",
    "# Fill missing values in 'workclass' column with mode\n",
    "mode_workclass = data['workclass'].mode()[0]\n",
    "data['workclass'].fillna(mode_workclass, inplace=True)\n",
    "\n",
    "# Fill missing values in 'occupation' column with mode\n",
    "mode_occupation = data['occupation'].mode()[0]\n",
    "data['occupation'].fillna(mode_occupation, inplace=True)\n",
    "\n",
    "# Fill missing values in 'native.country' column with mode\n",
    "mode_native_country = data['native_country'].mode()[0]\n",
    "data['native_country'].fillna(mode_native_country, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values again\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find categorical variables\n",
    "\n",
    "categorical = [var for var in data.columns if data[var].dtype=='O']\n",
    "\n",
    "data[categorical].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for cardinality in categorical variables\n",
    "for var in categorical:\n",
    "    print(var, ' contains ', len(data[var].unique()), ' labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get numerical variables\n",
    "numerical = [var for var in data.columns if data[var].dtype!='O']\n",
    "print('There are {} numerical variables\\n'.format(len(numerical)))\n",
    "print('The numerical variables are :', numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert columns with numerical data to numeric data type\n",
    "numerical_cols = ['age', 'fnlwgt', 'education_num', 'capital_gain', 'capital_loss', 'hours_per_week']\n",
    "data[numerical_cols] = data[numerical_cols].apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[numerical_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrmat = data[numerical_cols].corr()\n",
    "print(corrmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "heatmap = sns.heatmap(corrmat, vmin=-1, vmax=1, annot=True)\n",
    "heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features and target label\n",
    "income_raw = data['income']\n",
    "features_raw = data.drop('income', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distribution of income\n",
    "\"\"\"sns.countplot(x='income', data=data)\n",
    "plt.title('Distribution of Income')\n",
    "plt.xlabel('Income')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\"\"\"\n",
    "\n",
    "# Visualize the distribution of age\n",
    "\"\"\"sns.histplot(data['age'], bins=20)\n",
    "plt.title('Distribution of Age')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\"\"\"\n",
    "\n",
    "# Comparison Bayes-EBM: Visualize the income distribution by age\n",
    "sns.countplot(x='age', hue='income', data=data)\n",
    "plt.title('Income Distribution by Age')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "# Set x-axis ticks in steps of 10\n",
    "plt.xticks(range(0, max(data['age']) + 1, 10))\n",
    "plt.legend(title='Income', loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison Bayes-EBM: Visualize the income distribution by education level\n",
    "sns.countplot(x='education', hue='income', data=data)\n",
    "plt.title('Income Distribution by Education Level')\n",
    "plt.xlabel('Education Level')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Income', loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "# Visualize the income distribution by occupation\n",
    "\"\"\"sns.countplot(x='occupation', hue='income', data=data)\n",
    "plt.title('Income Distribution by Occupation')\n",
    "plt.xlabel('Occupation')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Income', loc='upper right')\n",
    "plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the income distribution by race\n",
    "\"\"\"sns.countplot(x='race', hue='income', data=data)\n",
    "plt.title('Income Distribution by Race')\n",
    "plt.xlabel('Race')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Income', loc='upper right')\n",
    "plt.show()\"\"\"\n",
    "\n",
    "\n",
    "# Visualize the income distribution by workclass\n",
    "\"\"\"sns.countplot(x='workclass', hue='income', data=data)\n",
    "plt.title('Income Distribution by Workclass')\n",
    "plt.xlabel('Workclass')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Income', loc='upper right')\n",
    "plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log-transform the skewed features\n",
    "skewed = ['capital_gain', 'capital_loss']\n",
    "features_log_transformed = pd.DataFrame(data = features_raw)\n",
    "features_log_transformed[skewed] = features_raw[skewed].apply(lambda x: np.log(x + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distribution of capital-gain after transformation\n",
    "\"\"\"sns.histplot(features_log_transformed['capital_gain'], bins=20)\n",
    "plt.title('Distribution of Capital Gain')\n",
    "plt.xlabel('Capital Gain')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Visualize the distribution of capital-loss after transformation\n",
    "sns.histplot(features_log_transformed['capital_loss'], bins=20)\n",
    "plt.title('Distribution of Capital Loss')\n",
    "plt.xlabel('Capital Loss')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Initialize a scaler, then apply it to the features\n",
    "scaler = MinMaxScaler()\n",
    "numerical = ['age', 'education_num', 'capital_gain', 'capital_loss', 'hours_per_week']\n",
    "\n",
    "features_log_minmax_transform = pd.DataFrame(data)\n",
    "features_log_minmax_transform[numerical] = scaler.fit_transform(features_log_minmax_transform[numerical])\n",
    "# Show an example of a record with scaling applied\n",
    "display(features_log_minmax_transform.head(n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing categorial features\n",
    "\n",
    "features_log_minmax_transform.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform Categorial into Numerical\n",
    "# One-hot encode the 'features_log_minmax_transform' data \n",
    "features_final = pd.get_dummies(features_log_minmax_transform)\n",
    "\n",
    "# Encode the 'income_raw' data to numerical values\n",
    "income = income_raw.map({'<=50K':0,'>50K':1})\n",
    "\n",
    "# Print the number of features after one-hot encoding\n",
    "encoded = list(features_final.columns)\n",
    "print(\"{} total features after one-hot encoding.\".format(len(encoded)))\n",
    "\n",
    "# See the encoded feature names\n",
    "print (encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>3. Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Declare feature vector and target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(['income'], axis=1)\n",
    "\n",
    "y = data['income']\n",
    "y = y.map({'<=50K':0, '>50K':1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Split data into separate training and test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split X and y into training and testing sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shape of X_train and X_test\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data types in X_train\n",
    "X_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display categorical variables\n",
    "categorical = [col for col in X_train.columns if X_train[col].dtypes == 'O']\n",
    "categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display numerical variables\n",
    "numerical = [col for col in X_train.columns if X_train[col].dtypes != 'O']\n",
    "numerical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Encode categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import category encoders\n",
    "import category_encoders as ce\n",
    "\n",
    "# encode remaining variables with one-hot encoding\n",
    "encoder = ce.OneHotEncoder(cols=['workclass', 'education', 'marital_status', 'occupation', 'relationship', \n",
    "                                 'race', 'sex', 'native_country'])\n",
    "\n",
    "X_train = encoder.fit_transform(X_train)\n",
    "X_test = encoder.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison Bayes-EBM:\n",
    "#marginal plot for provided data\n",
    "#how is the training data distributed over a specific range of value\n",
    "#Pearson correlation coefficient to interpret the linear relationship of these features with respect to the target\n",
    "graph = sns.jointplot(data= data, x=X_train['age'], y=y_train, alpha = 0.1)\n",
    "r, p = stats.pearsonr(X_train['age'], y_train)\n",
    "phantom, = graph.ax_joint.plot([], [], linestyle=\"\", alpha=0)\n",
    "graph.ax_joint.legend([phantom],['r={:f}, p={:f}'.format(r,p)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = X_train.columns\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "scaler = RobustScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_train = pd.DataFrame(X_train, columns=[cols])\n",
    "X_test = pd.DataFrame(X_test, columns=[cols])\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>4. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a Gaussian Naive Bayes classifier on the training set\n",
    "# instantiate the model\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# fit the model\n",
    "gnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 5. Explainability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Directed Acyclic Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import networkx as nx\n",
    "\n",
    "# Get mean for each feature and class\n",
    "means = gnb.theta_\n",
    "\n",
    "# Get feature names\n",
    "feature_names = X.columns\n",
    "\n",
    "# Create a directed graph using networkx\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Add nodes for features\n",
    "for feature in feature_names:\n",
    "    G.add_node(feature)\n",
    "\n",
    "# Add edges based on conditional dependencies\n",
    "for i, feature in enumerate(feature_names):\n",
    "    for j, class_label in enumerate(['0', '1']):\n",
    "        mean_label = f'Mean_{class_label}'\n",
    "        G.add_node(mean_label)\n",
    "        G.add_edge(mean_label, feature, label=f'Mean: {means[j, i]:.2f}')\n",
    "\n",
    "# Visualize the graph\n",
    "pos = nx.spring_layout(G)\n",
    "labels = nx.get_edge_attributes(G, 'label')\n",
    "nx.draw(G, pos, with_labels=True, font_weight='bold', node_size=2000, node_color='lightblue', font_size=8)\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels=labels)\n",
    "plt.show()\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Directed Acyclic Graph with weighted mean score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Get mean for each feature and class\n",
    "means = gnb.theta_\n",
    "\n",
    "# Get feature names\n",
    "feature_names = X.columns\n",
    "\n",
    "# Create a directed graph using networkx\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Add nodes for features\n",
    "for feature in feature_names:\n",
    "    G.add_node(feature)\n",
    "\n",
    "# Dictionary to store the weighted mean absolute score for each feature\n",
    "weighted_mean_absolute_scores = {}\n",
    "\n",
    "# Add edges based on conditional dependencies and calculate weighted mean absolute score for each feature\n",
    "for i, feature in enumerate(feature_names):\n",
    "    weighted_mean_absolute_score = 0.0\n",
    "    for j, class_label in enumerate(['0', '1']):\n",
    "        mean_label = f'Mean_{class_label}'\n",
    "        G.add_node(mean_label)\n",
    "        weight = means[j, i]  # Use the mean as the weight\n",
    "        weighted_mean_absolute_score += np.abs(weight)  # Accumulate absolute weights\n",
    "        G.add_edge(mean_label, feature, label=f'Mean: {means[j, i]:.4f}')\n",
    "\n",
    "         # Reverse the direction of the arrow by swapping the order of nodes\n",
    "        #G.add_edge(feature, mean_label, label=f'Mean: {means[j, i]:.4f}')\n",
    "\n",
    "    # Store the weighted mean absolute score for the current feature\n",
    "    weighted_mean_absolute_scores[feature] = weighted_mean_absolute_score\n",
    "\n",
    "# Display the weighted mean absolute score for each feature in descending order\n",
    "sorted_scores = sorted(weighted_mean_absolute_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "for feature, score in sorted_scores:\n",
    "    print(f\"Feature: {feature}, Weighted Mean Absolute Score: {score:.4f}\")\n",
    "\n",
    "# Visualize the graph\n",
    "pos = nx.spring_layout(G)\n",
    "labels = nx.get_edge_attributes(G, 'label')\n",
    "nx.draw(G, pos, with_labels=True, font_weight='bold', node_size=2000, node_color='lightblue', font_size=8)\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels=labels)\n",
    "plt.show()\n",
    "\n",
    "# Comparison Bayes-EBM:\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Directed Acyclic Graph: Dependencies betweeen Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import bnlearn as bn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have your data and trained DAG\n",
    "DAG = bn.structure_learning.fit(data)\n",
    "model_mle = bn.parameter_learning.fit(DAG, data, methodtype='maximumlikelihood')\n",
    "\n",
    "# Plotting the DAG with modified axis labels and title\n",
    "plt.figure(figsize=(8, 6))  # Define the figure size\n",
    "\n",
    "# Plot the DAG\n",
    "bn.plot(model_mle)\n",
    "\n",
    "# Retrieve the current Axes object\n",
    "ax = plt.gca()\n",
    "\n",
    "ax.set_title('bnlearn Directed Acyclic Graph (DAG)')  # Set the title\n",
    "ax.set_xlabel('Modified X-axis label')  # Set the modified X-axis label\n",
    "ax.set_ylabel('Modified Y-axis label')  # Set the modified Y-axis label\n",
    "\n",
    "plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Bedingte Wahrscheinlichkeiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# Select the relevant features for the specific scenario\n",
    "specific_data = pd.DataFrame({'education': ['Bachelors'], 'age': [20]})\n",
    "\n",
    "# Convert column names to strings\n",
    "specific_data.columns = specific_data.columns.astype(str)\n",
    "\n",
    "# Combine specific data with the original dataset to ensure all categories are present\n",
    "combined_data = pd.concat([data[['education', 'age']], specific_data])\n",
    "\n",
    "# Use OneHotEncoder for categorical variables\n",
    "encoder = OneHotEncoder(sparse=False) \n",
    "encoder.fit(combined_data)\n",
    "\n",
    "# Transform the specific data into a DataFrame\n",
    "specific_data_encoded = encoder.transform(specific_data)\n",
    "column_names = encoder.get_feature_names_out(specific_data.columns)\n",
    "specific_data_encoded_df = pd.DataFrame(specific_data_encoded, columns=column_names)\n",
    "\n",
    "\n",
    "if specific_data_encoded_df.shape[1] < 105:\n",
    "    missing_columns = 105 - specific_data_encoded_df.shape[1]\n",
    "    for i in range(missing_columns):\n",
    "        specific_data_encoded_df[f'extra_feature_{i}'] = 0\n",
    "\n",
    "# Calculate probabilities for the classes \n",
    "probabilities = gnb.predict_proba(specific_data_encoded_df)\n",
    "print(\"Class probabilities as percentages:\")\n",
    "probabilities_percent = probabilities * 100\n",
    "for class_idx, prob in enumerate(probabilities_percent[0]):\n",
    "    print(f\"Class {class_idx}: {prob:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "# Your existing code for creating the graph 'G'\n",
    "\n",
    "# Assign arbitrary z-coordinates for 3D visualization\n",
    "z_coords = {node: 0 for node in G.nodes()}  # Assigning a default value for z-coordinate (you may need to adjust this)\n",
    "\n",
    "# Plotting in 3D\n",
    "fig = plt.figure(figsize=(12, 10))  # Increase figure size\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Get x, y, z coordinates for nodes\n",
    "nodes = G.nodes()\n",
    "feature_nodes = [node for node in nodes if isinstance(node, str) and 'Mean' not in node]\n",
    "\n",
    "# Create a colormap to generate distinct colors for each feature\n",
    "colormap = plt.cm.get_cmap('tab20', len(feature_nodes))\n",
    "\n",
    "# Plot nodes with different colors for each feature\n",
    "for idx, node in enumerate(feature_nodes):\n",
    "    color = colormap(idx)\n",
    "    ax.scatter(pos[node][0], pos[node][1], z_coords[node], color=color, s=800, label=node)  # Increase marker size\n",
    "\n",
    "# Plot edges\n",
    "for edge in G.edges():\n",
    "    ax.plot([pos[edge[0]][0], pos[edge[1]][0]], [pos[edge[0]][1], pos[edge[1]][1]], [z_coords[edge[0]], z_coords[edge[1]]], 'k-')\n",
    "\n",
    "# Add labels\n",
    "for node in nodes:\n",
    "    ax.text(pos[node][0], pos[node][1], z_coords[node], node, color='black', fontsize=10)  # Increase label font size\n",
    "\n",
    "ax.set_xlabel('X-axis')\n",
    "ax.set_ylabel('Y-axis')\n",
    "ax.set_zlabel('Z-axis')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Assuming you have 'age' and 'sex' as features\n",
    "new_data = pd.DataFrame({'age': [25], 'sex': ['Female']})  # Adjust the values accordingly\n",
    "\n",
    "# Encoding categorical variables like 'sex' to numeric values\n",
    "new_data['sex'] = new_data['sex'].map({'Male': 0, 'Female': 1})  # Assuming binary encoding\n",
    "\n",
    "# If 'gnb' expects 105 features, create dummy features for the rest\n",
    "all_features = pd.DataFrame()\n",
    "\n",
    "for i in range(3, 106):\n",
    "    all_features[f'feature_{i}'] = [0]  # Initialize all other features with 0\n",
    "\n",
    "# Update 'age' and 'sex' in all_features with the provided values\n",
    "all_features['age'] = new_data['age']\n",
    "all_features['sex'] = new_data['sex']\n",
    "\n",
    "# Predict the probabilities for the new_data instance\n",
    "predicted_probabilities = gnb.predict_proba(all_features)\n",
    "\n",
    "# Assuming your classes or labels are ['<=50K', '>50K']\n",
    "classes = gnb.classes_\n",
    "\n",
    "# Print the probabilities for each income category\n",
    "for idx, label in enumerate(classes):\n",
    "    print(f\"Probability of {label} income: {predicted_probabilities[0][idx]}\")\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get mean for each feature and class\n",
    "means = gnb.theta_\n",
    "\n",
    "# Get feature names\n",
    "feature_names = X.columns\n",
    "\n",
    "# Plot the mean values\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i, feature in enumerate(feature_names):\n",
    "    plt.barh([f'{feature}_class_0', f'{feature}_class_1'], means[:, i], label=feature)\n",
    "\n",
    "plt.xlabel('Mean Value')\n",
    "plt.title('Feature Importance (Mean Values)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "# Get mean for each feature and class\n",
    "means = gnb.theta_\n",
    "\n",
    "# Get feature names\n",
    "feature_names = X.columns\n",
    "\n",
    "# Create a directed graph using networkx\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Dictionary to store the weighted mean absolute score for each feature\n",
    "weighted_mean_absolute_scores = {}\n",
    "\n",
    "# Add edges based on conditional dependencies and calculate weighted mean absolute score for each feature\n",
    "for i, feature in enumerate(feature_names):\n",
    "    weighted_mean_absolute_score = 0.0\n",
    "    for j, class_label in enumerate(['0', '1']):\n",
    "        mean_label = f'Mean_{class_label}'\n",
    "        G.add_node(mean_label)\n",
    "        weight = means[j, i]  # Use the mean as the weight\n",
    "        weighted_mean_absolute_score += np.abs(weight)  # Accumulate absolute weights\n",
    "        G.add_edge(mean_label, feature, label=f'Mean: {means[j, i]:.4f}')\n",
    "\n",
    "    # Store the weighted mean absolute score for the current feature\n",
    "    weighted_mean_absolute_scores[feature] = weighted_mean_absolute_score\n",
    "\n",
    "# Display the weighted mean absolute score for each feature in a horizontal bar diagram\n",
    "sorted_scores = sorted(weighted_mean_absolute_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "features, scores = zip(*sorted_scores)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(features, scores, color='lightblue', edgecolor='black', alpha=0.8)\n",
    "plt.xlabel('Weighted Mean Absolute Score')\n",
    "plt.title('Feature Importance (Weighted Mean Absolute Scores)')\n",
    "plt.show()\n",
    "\n",
    "# Comparison Bayes-EBM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get mean for each feature and class\n",
    "means = gnb.theta_\n",
    "\n",
    "# Get feature names\n",
    "feature_names = X.columns\n",
    "\n",
    "# Plot the mean values\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i, feature in enumerate(feature_names):\n",
    "    plt.barh(feature, means[:, i])\n",
    "\n",
    "plt.xlabel('Mean Value')\n",
    "plt.title('Feature Importance (Mean Values)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Partial Dependence Plots (PDPs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_name = 'age'\n",
    "\n",
    "# Create the PDP\n",
    "unique_values = np.unique(X_test[feature_name])\n",
    "pdp_values = []\n",
    "\n",
    "for value in unique_values:\n",
    "    X_pdp = X_test.copy()\n",
    "    X_pdp[feature_name] = value\n",
    "    pdp_values.append(gnb.predict_proba(X_pdp)[:, 1].mean())\n",
    "\n",
    "# Plot the PDP\n",
    "plt.plot(unique_values, pdp_values, marker='o')\n",
    "plt.xlabel(feature_name)\n",
    "plt.ylabel('Average Predicted Probability')\n",
    "plt.title(f'Partial Dependence Plot for {feature_name}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>6. Predictions and Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Evaluation\n",
    "# Counting the ones as this is the naive case. Note that 'income' is the 'income_raw' data encoded to numerical values done in the data preprocessing step.\n",
    "TP = np.sum(income) \n",
    "# Specific to the naive case\n",
    "FP = income.count() - TP\n",
    "# No predicted negatives in the naive case\n",
    "TN = 0 \n",
    "FN = 0 \n",
    "\n",
    "# Calculate accuracy, precision and recall\n",
    "accuracy = TP / (TP + FP + TN + FN)\n",
    "recall = TP / (TP + FN)\n",
    "precision = TP / (TP + FP)\n",
    "\n",
    "# Calculate F-score using the formula above for beta = 0.5 and correct values for precision and recall.\n",
    "beta = 0.5\n",
    "fscore = (1 + beta**2) * ((precision * recall) / ((beta**2) * precision + recall))\n",
    "\n",
    "# Print the results \n",
    "print(\"Naive Predictor: [Accuracy score: {:.4f}, F-score: {:.4f}]\".format(accuracy, fscore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Predict the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gnb.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Check accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model accuracy score: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the train-set and test-set accuracy\n",
    "y_pred_train = gnb.predict(X_train)\n",
    "y_pred_train\n",
    "print('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for overfitting and underfitting\n",
    "# print the scores on training and test set\n",
    "\n",
    "print('Training set score: {:.4f}'.format(gnb.score(X_train, y_train)))\n",
    "\n",
    "print('Test set score: {:.4f}'.format(gnb.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model accuracy with null accuracy\n",
    "# check class distribution in test set\n",
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions using the model\n",
    "predictions = (gnb.fit(X_train, y_train)).predict(X_test)\n",
    "\n",
    "# Report accuracy and fscore\n",
    "print(\"Accuracy score on testing data {:.4f}\".format(accuracy_score(y_test, predictions)))\n",
    "print(\"F-score on testing data: {:.4f}\".format(fbeta_score(y_test, predictions, beta = 0.5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the Confusion Matrix and slice it into four pieces\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion matrix\\n\\n', cm)\n",
    "print('\\nTrue Positives(TP) = ', cm[0,0])\n",
    "print('\\nTrue Negatives(TN) = ', cm[1,1])\n",
    "print('\\nFalse Positives(FP) = ', cm[0,1])\n",
    "print('\\nFalse Negatives(FN) = ', cm[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize confusion matrix with seaborn heatmap\n",
    "cm_matrix = pd.DataFrame(data=cm, columns=['Actual Positive:1', 'Actual Negative:0'], \n",
    "                                 index=['Predict Positive:1', 'Predict Negative:0'])\n",
    "sns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Classification metrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = cm[0,0]\n",
    "TN = cm[1,1]\n",
    "FP = cm[0,1]\n",
    "FN = cm[1,0]\n",
    "\n",
    "# print classification accuracy\n",
    "classification_accuracy = (TP + TN) / float(TP + TN + FP + FN)\n",
    "print('Classification accuracy : {0:0.4f}'.format(classification_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print classification error\n",
    "classification_error = (FP + FN) / float(TP + TN + FP + FN)\n",
    "print('Classification error : {0:0.4f}'.format(classification_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Class probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = gnb.predict_proba(X_test)[0:10]\n",
    "# store the probabilities in dataframe\n",
    "y_pred_prob_df = pd.DataFrame(data=y_pred_prob, columns=['Prob of - <=50K', 'Prob of - >50K'])\n",
    "\n",
    "# store the predicted probabilities for class 1 - Probability of >50K\n",
    "y_pred1 = gnb.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histogram of predicted probabilities\n",
    "\n",
    "# adjust the font size \n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# plot histogram with 10 bins\n",
    "plt.hist(y_pred1, bins = 10)\n",
    "\n",
    "# set the title of predicted probabilities\n",
    "plt.title('Histogram of predicted probabilities of salaries >50K')\n",
    "\n",
    "# set the x-axis limit\n",
    "plt.xlim(0,1)\n",
    "\n",
    "# set the title\n",
    "plt.xlabel('Predicted probabilities of salaries >50K')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine X_train and y_train into a single DataFrame\n",
    "train_data = pd.concat([X_train, pd.Series(y_train, name='Target')], axis=1)\n",
    "# Calculate standard deviations for each feature and class\n",
    "std_devs = train_data.groupby('Target').std()\n",
    "# Print the standard deviations\n",
    "print(\"Standard Deviations:\")\n",
    "print(std_devs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 7. Hyperparametertuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.logspace(0,-9, num=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "cv_method = RepeatedStratifiedKFold(n_splits=5, \n",
    "                                    n_repeats=3, \n",
    "                                    random_state=999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_NB = {'var_smoothing': np.logspace(0,-9, num=100)}\n",
    "\n",
    "gs_NB = GridSearchCV(estimator=gnb, \n",
    "                     param_grid=params_NB, \n",
    "                     cv=cv_method,\n",
    "                     verbose=1, \n",
    "                     scoring='accuracy')\n",
    "\n",
    "\n",
    "gs_NB.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_NB.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_NB.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_NB = pd.DataFrame(gs_NB.cv_results_['params'])\n",
    "results_NB['test_score'] = gs_NB.cv_results_['mean_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(results_NB['var_smoothing'], results_NB['test_score'], marker = '.')    \n",
    "plt.xlabel('Var. Smoothing')\n",
    "plt.ylabel(\"Mean CV Score\")\n",
    "plt.title(\"NB Performance Comparison\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 8. Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "param_distribution = {'var_smoothing': np.logspace(0, -9, num=100)}\n",
    "scoring = 'accuracy'  \n",
    "\n",
    "for i in range(1, 13):\n",
    "    # PCA durchführen\n",
    "    pca = PCA(n_components=i)\n",
    "    X_train_pca = pca.fit_transform(X_train)\n",
    "    \n",
    "    # RandomizedSearchCV für den Naive Bayes Classifier\n",
    "    search_cv = RandomizedSearchCV(GaussianNB(), param_distribution, scoring=scoring, n_jobs=-1,\n",
    "                                   cv=StratifiedKFold(n_splits=10, shuffle=True), refit=scoring)\n",
    "    search_cv.fit(X_train_pca, y_train)\n",
    "    best_model = search_cv.best_estimator_\n",
    "\n",
    "    # Testdaten transformieren\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "    \n",
    "    # Vorhersagen\n",
    "    y_pred = best_model.predict(X_test_pca)\n",
    "    \n",
    "    # Modellbewertung\n",
    "    f1 = fbeta_score(y_test, y_pred, beta=1, pos_label=1)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"{i} {acc} {f1}\")\n",
    "    \n",
    "    # Ergebnisse speichern\n",
    "    result.append((i, acc, f1, pca, best_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finde den Index der Zeile mit der höchsten Genauigkeit\n",
    "best_index = np.argmax(np.array(result)[:, 1])\n",
    "\n",
    "# Extrahiere die Informationen für die beste Zeile\n",
    "best_row = result[best_index]\n",
    "\n",
    "# Extrahiere die Anzahl der Hauptkomponenten und die PCA-Objekt\n",
    "best_components_count = best_row[0]\n",
    "best_pca = best_row[3]\n",
    "\n",
    "# Extrahiere die Namen der drei Hauptkomponenten mit den höchsten Ladevektoren (Betrag)\n",
    "top_indices = np.argsort(np.abs(best_pca.components_))[:, -3:]\n",
    "top_feature_names = X_train.columns[top_indices.flatten()]\n",
    "\n",
    "# Gib die Informationen aus\n",
    "print(\"Beste Zeile mit höchster Genauigkeit:\")\n",
    "print(f\"Anzahl der Hauptkomponenten: {best_components_count}\")\n",
    "print(f\"Namen der Hauptkomponenten mit höchstem Betrag der Ladevektoren:\")\n",
    "print(top_feature_names)\n",
    "print(f\"Genauigkeit: {best_row[1]}\")\n",
    "print(f\"F1-Score: {best_row[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrahiere die eindeutigen Namen der Merkmale\n",
    "unique_feature_names = set([name[0] for name in top_feature_names])\n",
    "\n",
    "# Gib die eindeutigen Namen der Merkmale aus\n",
    "print(\"Eindeutige Namen der Hauptkomponenten:\")\n",
    "print(unique_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loadings Matrix für die ersten drei Hauptkomponenten\n",
    "loadings_matrix = pca.components_\n",
    "\n",
    "# Drucke die Loadings Matrix\n",
    "print(\"Loadings Matrix:\")\n",
    "print(loadings_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot für jede Hauptkomponente\n",
    "for i in range(loadings_matrix.shape[0]):\n",
    "    plt.figure(figsize=(8, 2))\n",
    "    plt.bar(range(len(loadings_matrix[i])), loadings_matrix[i])\n",
    "    plt.title(f'Loadings for PC{i+1}')\n",
    "    plt.xlabel('Feature Name')\n",
    "    plt.ylabel('Loading Value')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ex01",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
